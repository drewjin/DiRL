<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="DiRL: An Efficient Post-Training Framework for Diffusion Language Models - Ying Zhu, Jiaxin Wan, Xiaoran Liu, Siyang He, Qiqi Wang, Xu Guo, Tianyi Liang, Zengfeng Huang, Ziwei He, Xipeng Qiu">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="KEYWORD1, KEYWORD2, KEYWORD3, machine learning, computer vision, AI">
  <!-- TODO: List all authors -->
  <meta name="author" content="Ying Zhu, Jiaxin Wan, Xiaoran Liu, Siyang He, Qiqi Wang, Xu Guo, Tianyi Liang, Zengfeng Huang, Ziwei He, Xipeng Qiu">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="INSTITUTION_OR_LAB_NAME">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="DiRL: An Efficient Post-Training Framework for Diffusion Language Models">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="PAPER_TITLE - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="Ying Zhu">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="KEYWORD1">
  <meta property="article:tag" content="KEYWORD2">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="DiRL: An Efficient Post-Training Framework for Diffusion Language Models">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="PAPER_TITLE - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="DiRL: An Efficient Post-Training Framework for Diffusion Language Models">
  <meta name="citation_author" content="Zhu, Ying">
  <meta name="citation_author" content="Wan, Jiaxin">
  <meta name="citation_author" content="Liu, Xiaoran">
  <meta name="citation_author" content="He, Siyanag">
  <meta name="citation_author" content="Wang, Qiqi">
  <meta name="citation_author" content="Guo, Xu">
  <meta name="citation_author" content="Liang, Tianyi">
  <meta name="citation_author" content="Huang, Zengfeng">
  <meta name="citation_author" content="He, Ziwei">
  <meta name="citation_author" content="Qiu, Xipeng">
  <meta name="citation_publication_date" content="2024">
  <meta name="citation_conference_title" content="CONFERENCE_NAME">
  <meta name="citation_pdf_url" content="https://arxiv.org/pdf/2512.22234.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>DiRL: An Efficient Post-Training Framework for Diffusion Language Models - Ying Zhu et al. | Academic Research</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- KaTeX for math rendering -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "DiRL: An Efficient Post-Training Framework for Diffusion Language Models",
    "description": "BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS",
    "author": [
      {
        "@type": "Person",
        "name": "Ying Zhu",
        "affiliation": {
          "@type": "Organization",
          "name": "Fudan University, Shanghai Innovation Institute"
        }
      },
      {
        "@type": "Person",
        "name": "Jiaxin Wan",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Electronic Science and Technology of China, Shanghai Innovation Institute"
        }
      },
      {
        "@type": "Person",
        "name": "Xiaoran Liu",
        "affiliation": {
          "@type": "Organization",
          "name": "Fudan University, Shanghai Innovation Institute"
        }
      },
      {
        "@type": "Person",
        "name": "Siyang He",
        "affiliation": {
          "@type": "Organization",
          "name": "Fudan University, Shanghai Innovation Institute"
        }
      },
      {
        "@type": "Person",
        "name": "Qiqi Wang",
        "affiliation": {
          "@type": "Organization",
          "name": "Fudan University, Shanghai Innovation Institute"
        }
      },
      {
        "@type": "Person",
        "name": "Xu Guo",
        "affiliation": {
          "@type": "Organization",
          "name": "Fudan University, Shanghai Innovation Institute"
        }
      },
      {
        "@type": "Person",
        "name": "Tianyi Liang",
        "affiliation": {
          "@type": "Organization",
          "name": "Shanghai Innovation Institute"
        }
      },
      {
        "@type": "Person",
        "name": "Zengfeng Huang",
        "affiliation": {
          "@type": "Organization",
          "name": "Fudan University, Shanghai Innovation Institute"
        }
      },
      {
        "@type": "Person",
        "name": "Ziwei He",
        "affiliation": {
          "@type": "Organization",
          "name": "Shanghai Innovation Institute"
        }
      },
      {
        "@type": "Person",
        "name": "Xipeng Qiu",
        "affiliation": {
          "@type": "Organization",
          "name": "Fudan University, Shanghai Innovation Institute"
        }
      }
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "CONFERENCE_OR_JOURNAL_NAME"
    },
    "url": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["KEYWORD1", "KEYWORD2", "KEYWORD3", "machine learning", "computer vision"],
    "abstract": "FULL_ABSTRACT_TEXT_HERE",
    "citation": "BIBTEX_CITATION_HERE",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "RESEARCH_AREA_1"
      },
      {
        "@type": "Thing", 
        "name": "RESEARCH_AREA_2"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "INSTITUTION_OR_LAB_NAME",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-1 publication-title">DiRL: An Efficient Post-Training Framework for Diffusion Language Models</h1>
            <div class="is-size-5 publication-authors">
              <!-- TODO: Replace with your paper authors and their personal links -->
              <span class="author-block">
                <a href="#" target="_blank">Ying Zhu</a><sup>1,2,3</sup>,</span>
                <span class="author-block">
                  <a href="#" target="_blank">Jiaxin Wan</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="#" target="_blank">Xiaoran Liu</a><sup>1,2,3</sup>,</span>
                  <span class="author-block">
                    <a href="#" target="_blank">Siyang He</a><sup>1,2,3</sup>,</span>
                  <span class="author-block">
                    <a href="#" target="_blank">Qiqi Wang</a><sup>1,2,3</sup>,</span>
                  <span class="author-block">
                    <a href="#" target="_blank">Xu Guo</a><sup>1,2</sup>,</span>
                  <span class="author-block">
                    <a href="#" target="_blank">Tianyi Liang</a><sup>2,3</sup>,</span>
                  <span class="author-block">
                    <a href="#" target="_blank">Zengfeng Huang</a><sup>1,2</sup>,</span>
                  <span class="author-block">
                    <a href="#" target="_blank">Ziwei He</a><sup>2,3,â€ </sup>,</span>
                  <span class="author-block">
                    <a href="#" target="_blank">Xipeng Qiu</a><sup>1,2,â€ </sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <!-- TODO: Replace with your institution and conference/journal info -->
                    <span class="author-block"><sup>1</sup>Fudan University, <sup>2</sup>Shanghai Innovation Institute, <sup>3</sup>OpenMoss Team</span>
                    <br>
                    <span class="author-block" style="font-size: 0.9em;"><sup>â€ </sup>Corresponding authors</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- arXiv Link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2512.22234" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>arXiv</span>
                      </a>
                    </span>

                         <!-- GitHub Link -->
                      <span class="link-block">
                        <a href="https://github.com/OpenMOSS/DiRL" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <img src="https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png" alt="GitHub" style="width: 1.2em; height: 1.2em; vertical-align: middle; filter: invert(1);">
                        </span>
                        <span>GitHub</span>
                      </a>
                    </span>

                    <!-- Hugging Face Model Link -->
                    <span class="link-block">
                      <a href="https://huggingface.co/OpenMOSS-Team/DiRL-8B-Instruct" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <img src="https://huggingface.co/front/assets/huggingface_logo.svg" alt="Hugging Face" style="width: 1.2em; height: 1.2em; vertical-align: middle;">
                      </span>
                      <span>Model</span>
                    </a>
                  </span>

                    <!-- Hugging Face Data Link -->
                    <span class="link-block">
                      <a href="https://huggingface.co/collections/Auraithm/dirl" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <img src="https://huggingface.co/front/assets/huggingface_logo.svg" alt="Hugging Face" style="width: 1.2em; height: 1.2em; vertical-align: middle;">
                      </span>
                      <span>Data</span>
                    </a>
                  </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser Image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- Main Teaser Image -->
      <figure class="image">
        <img src="static/images/accuracy.svg" alt="Aha Framework Overview" style="width: 100%; border-radius: 12px; box-shadow: 0 8px 16px rgba(0,0,0,0.15);"/>
      </figure>
    </div>
  </div>
</section>
<!-- End teaser image -->

<!-- TL;DR -->
<section class="hero is-small"  style="padding: 8rem 5rem;">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;">TL;DR</h2>
        <div class="content" style="font-size: 1.2rem; font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; line-height: 1.7; text-align: justify;">
          <p>We introduce <strong style="font-weight: 600;">DiRL</strong>, an open-source training framework for Diffusion Language Models (DLLMs) with SFT and RL stages. Using this framework, we train <strong style="font-weight: 600;">DiRL-8B-Instruct</strong>, achieving state-of-the-art results at the 8B scale on mathematical reasoning benchmarks, even outperforming 32B models on most tasks.</p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End TL;DR -->


<!-- HighLights Section -->
<section class="hero is-small is-light" style="padding: 5rem 1.5rem;">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered" style="margin-bottom: 0.5rem; font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;">HighLights</h2>
    <div class="content" style="font-size: 1.2rem; font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; line-height: 1.7;">
      <ul style="line-height: 1.8; margin-bottom: 0; list-style-position: outside; padding-left: 1.5rem;">
        <li style="margin-bottom: 0.75rem;"><strong style="font-weight: 600;">ðŸŽ¯ Novel RL Algorithm:</strong> We propose <strong>DiPO (Discrete Diffusion Policy Optimization)</strong>, an RL algorithm that optimizes at the generation step level for DLLMs. It achieves unbiased implementation with complete consistency between optimization objectives and training process, and integrates dynamic sampling from DAPO during rollout to filter out low-quality data.</li>
        <li style="margin-bottom: 0.75rem;"><strong style="font-weight: 600;">ðŸš€ Efficient Training & Inference:</strong> We support <strong>Accelerate</strong> framework for distributed training and <strong>Flex-Attention</strong> for training acceleration, while implementing <strong>LMDeploy</strong>-based <strong>inference server</strong> for efficient rollout with <strong>online policy updates</strong>, ensuring training-inference consistency without offline model reloading.</li>
        <li><strong style="font-weight: 600;">ðŸ§  SOTA Performance:</strong> We achieve state-of-the-art results at the 8B scale among both autoregressive (AR) models and diffusion language models (DLLMs) across multiple mathematical reasoning benchmarks. Specifically, we reach <strong>83.05%</strong> on MATH500, <strong>20.63%</strong> on AIME2024, and <strong>20.83%</strong> on AIME2025, surpassing all 8B baselines and even outperforming the 32B Qwen2.5-32B-Instruct model on AIME benchmarks.</li>
      </ul>
    </div>
  </div>
</section>
<!-- End HighLights Section -->



























<!-- Teaser trace-->
<section class="hero teaser" style="padding: 3rem 5rem;">
  <div class="container" style="max-width: 1300px !important; width: 1300px !important;">
    <div class="hero-body">
        <style>
        :root {
            --fg: #111;
            --bg: #fafafa;
            --muted: #888;
            --new: #0a7f2e;
            --mask: #aaa;
            --border: #ddd;
            --prompt-bg: transparent;
            --output-bg: #fdfdfd;
            --prompt-border: #e0e0e0;
            --output-border: #c4e3c4;
        }
        .teaser .hero-body {
            font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
            color: var(--fg);
        }
        .teaser h1 {
            font-size: 20px;
            margin: 0 0 12px 0;
            font-weight: 600;
            text-align: center;
            color: #333;
        }
        .teaser #meta {
            color: var(--muted);
            font-size: 13px;
            margin-bottom: 10px;
            text-align: center;
        }
        .teaser .section {
            border-radius: 12px;
            padding: 16px;
            margin-bottom: 20px;
            white-space: pre-wrap;
            overflow-wrap: anywhere;
            line-height: 1.55;
            box-shadow: 0 1px 2px rgba(0,0,0,0.05);
            width: 1300px !important;
            max-width: 1300px !important;
            box-sizing: border-box;
        }
        .teaser #prompt {
            background: var(--prompt-bg);
            border: 1px solid var(--prompt-border);
            min-height: 80px;
            padding: 16px 16px !important;
            width: 1300px !important;
        }
        .teaser .output-container {
            display: flex;
            gap: 20px;
            width: 1300px !important;
        }
        .teaser .output-box {
            flex: 1;
            min-width: 0;
        }
        .teaser .output-title {
            font-size: 18px;
            font-weight: 600;
            margin-bottom: 4px;
            text-align: center;
            color: #555;
        }
        .teaser .output-speed {
            font-size: 16px;
            color: var(--muted);
            margin-bottom: 8px;
            text-align: center;
        }
        .teaser #output, .teaser #output-autoregressive {
            background: var(--output-bg);
            border: 1px solid var(--output-border);
            min-height: 800px;
            height: 800px;
            overflow: hidden;
            padding: 16px 16px !important;
            width: 100% !important;
            box-sizing: border-box;
        }
        .teaser .masked {
            color: transparent;
            text-shadow: 0 0 0 var(--mask);
        }
        .teaser .unmasked {
            color: inherit;
        }
        .teaser .new {
            background: #e9f7ee;
            outline: 1px dashed #b7e1c3;
            animation: fadeIn 0.3s ease-out;
        }
        .teaser .prompt {
            color: #6b7280;
            font-weight: 500;
        }
        .teaser .special {
            color: var(--muted);
        }
        .teaser .maskToken {
            display:inline-block;
            width:9ch;
            text-align:center;
            opacity:0.6;
            white-space:nowrap;
        }
        @keyframes fadeIn {
            from { background:#cdecd8; }
            to { background:#e9f7ee; }
        }
        </style>

        <h2 class="title is-3 has-text-centered" style="font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; margin-bottom: 2rem;">Decoding Visualization</h2>
        
        <div id="prompt" class="section"></div>
        
        <div class="output-container">
            <div class="output-box">
                <div class="output-title">Diffusion Decoding (with MASK)</div>
                <div id="meta-parallel" class="output-speed"></div>
                <div id="output" class="section"></div>
            </div>
            <div class="output-box">
                <div class="output-title">Autoregressive Decoding</div>
                <div id="meta-autoregressive" class="output-speed"></div>
                <div id="output-autoregressive" class="section"></div>
            </div>
        </div>

        <script id="data" type="application/json">{"pieces": ["Prompt", ":", " ", "<|im_start|>", "user", "\n", "Find", " the", " sum", " of", " all", " integer", " bases", " $", "b", ">", "9", "$", " for", " which", " $", "1", "7", "_{", "b", "}$", " is", " a", " divisor", " of", " $", "9", "7", "_{", "b", "}$", ".\n", "Please", " reason", " step", " by", " step", ",", " and", " put", " your", " final", " answer", " within", " $\\", "boxed", "{}", "$.", "<|im_end|>", "\n", "<|im_start|>", "assistant", "\n", "Alright", ",", " I", " have", " this", " problem", ":\n\n", "**", "Find", " the", " sum", " of", " all", " integer", " bases", " \\(", " b", " >", " ", "9", " \\", ")", " for", " which", " \\(", " ", "1", "7", "_b", " \\", ")", " is", " a", " divisor", " of", " \\(", " ", "9", "7", "_b", " \\", ").", "**\n\n", "First", ",", " I", " need", " to", " understand", " what", " \\(", " ", "1", "7", "_b", " \\", ")", " and", " \\(", " ", "9", "7", "_b", " \\", ")", " mean", ".", " The", " subscript", " \\(", " b", " \\", ")", " indicates", " that", " these", " numbers", " are", " in", " base", " \\(", " b", " \\", ").", " \n\n", "###", " Step", " ", "1", ":", " Convert", " Base", " \\(", " b", " \\", ")", " Numbers", " to", " Decimal", "\n", "I", " need", " to", " express", " both", " numbers", " in", " terms", " of", " base", " ", "1", "0", " (", "decimal", ")", " to", " work", " with", " them", " easily", ".\n\n", "1", ".", " **", "Convert", " \\(", " ", "1", "7", "_b", " \\", ")", " to", " decimal", ":", "**\n", "  ", " \\", "[", " ", "1", "7", "_b", " =", " ", "1", " \\", "times", " b", "^", "1", " +", " ", "7", " \\", "times", " b", "^", "0", " =", " b", " +", " ", "7", " \\", "]\n\n", "2", ".", " **", "Convert", " \\(", " ", "9", "7", "_b", " \\", ")", " to", " decimal", ":", "**\n", "  ", " \\", "[", " ", "9", "7", "_b", " =", " ", "9", " \\", "times", " b", "^", "1", " +", " ", "7", " \\", "times", " b", "^", "0", " =", " ", "9", "b", " +", " ", "7", " \\", "]\n\n", "###", " Step", " ", "2", ":", " Understand", " the", " Div", "is", " Condition", "\n", "The", " problem", " states", " that", " \\(", " ", "1", "7", "_b", " \\", ")", " is", " a", " divisor", " of", " \\(", " ", "9", "7", "_b", " \\", ").", " In", " decimal", " terms", ",", " means", ":\n", "\\[", " b", " +", " ", "7", " \\", "text", "{", " divides", " }", " ", "9", "b", " +", " ", "7", " \\", "]\n\n", "In", " other", " words", ",", " \\(", " ", "9", "b", " +", " ", "7", " \\", ")", " is", " divisible", " exactly", " by", " \\(", " b", " +", " ", "7", " \\", ").", " \n\n", "###", " Step", " ", "3", ":", " Use", " Polynomial", " Division", " or", " Remain", "der", " The", "orem", "\n", "I", " recall", " that", " for", " divis", "ibility", " conditions", " like", " this", ",", " it", "'s", " helpful", " to", " perform", " polynomial", " division", " or", " use", " the", " Remain", "der", " The", "orem", ".\n\n", "**", "Let", "'s", " perform", " polynomial", " division", ":", "**\n\n", "Div", "ide", " \\(", " ", "9", "b", " +", " ", "7", " \\", ")", " by", " \\(", " b", " +", " ", "7", " \\", ").\n\n", "1", ".", " **", "First", " division", " step", ":", "**\n", "  ", " \\", "[", " \\", "frac", "{", "9", "b", "}{", "b", "}", " =", " ", "9", " \\", "]\n", "  ", " Multiply", " \\(", " ", "9", " \\", ")", " by", " \\(", " b", " +", " ", "7", " \\", "):", " \\(", " ", "9", "b", " +", " ", "6", "3", " \\", ")\n", "  ", " Subtract", " from", " \\(", " ", "9", "b", " +", " ", "7", " \\", "):\n", "  ", " \\", "[", " (", "9", "b", " +", " ", "7", ")", " -", " (", "9", "b", " +", " ", "6", "3", ")", " =", " -", "5", "6", " \\", "]\n\n", "So", ",", " the", " remainder", " is", " \\", "(-", "5", "6", "\\", ").\n\n", "But", " since", " \\(", " b", " +", " ", "7", " \\", ")", " must", " divide", " \\(", " ", "9", "b", " +", " ", "7", " \\", ")", " exactly", ",", " the", " remainder", " must", " be", " zero", ".", " However", ",", " here", " the", " remainder", " is", " \\", "(-", "5", "6", "\\", "),", " which", " is", " not", " zero", ".", " \n\n", "**", "But", " this", " seems", " contradictory", "!", "**", " The", " condition", " says", " that", " \\(", " b", " +", " ", "7", " \\", ")", " divides", " \\(", " ", "9", "b", " +", " ", "7", " \\", "),", " but", " according", " to", " the", " division", ",", " the", " remainder", " is", " non", "-zero", ".", " \n\n", "###", " Step", " ", "4", ":", " Re", "-e", "valuate", " the", " Approach", "\n", "Maybe", " I", " made", " a", " mistake", ".", " Instead", " of", " division", ",", " perhaps", " I", " should", " think", " about", " the", " condition", " in", " terms", " of", " divis", "ibility", ".\n\n", "The", " condition", " \\(", " b", " +", " ", "7", " \\", ")", " divides", " \\(", " ", "9", "b", " +", " ", "7", " \\", ")", " can", " be", " written", " as", ":\n", "\\[", " ", "9", "b", " +", " ", "7", " \\", "equiv", " ", "0", " \\", "mod", " (", " (", "b", " +", " ", "7", "))", " \\", "]\n\n", "But", " from", " the", " division", ",", " we", " saw", " that", ":\n", "\\[", " ", "9", "b", " +", " ", "7", " =", " (", "b", " +", " ", "7", ")", " \\", "times", " ", "9", " +", " (-", "5", "6", ")", " \\", "]\n\n", "Thus", ":\n", "\\[", " ", "9", "b", " +", " ", "7", " \\", "equiv", " -", "5", "6", " \\", "mod", " (", "b", " +", " ", "7", ")", " \\", "]\n\n", "But", " \\(", " b", " +", " ", "7", " \\", "equiv", " ", "0", " \\", "mod", " (", "b", " +", " ", "7", ")", " \\", "),", " so", ":\n", "\\[", " -", "5", "6", " \\", "equiv", " ", "0", " \\", "mod", " (", "b", " +", " ", "7", ")", " \\", "]\n\n", "Which", " implies", ":\n", "\\[", " b", " +", " ", "7", " \\", "text", "{", " divides", " }", " ", "5", "6", " \\", "]\n\n", "###", " Step", " ", "5", ":", " Find", " All", " Div", "isors", " of", " ", "5", "6", "\n", "Now", ",", " I", " need", " to", " find", " all", " positive", " div", "isors", " of", " ", "5", "6", " because", " \\(", " b", " +", " ", "7", " \\", ")", " must", " be", " positive", " (", "since", " \\(", " b", " >", " ", "9", " \\", "))", ".\n\n", "First", ",", " factor", "ize", " ", "5", "6", ":\n", "\\[", " ", "5", "6", " =", " ", "2", "^", "3", " \\", "times", " ", "7", " \\", "]\n\n", "The", " positive", " div", "isors", " are", ":\n", "\\[", " ", "1", ",", " ", "2", ",", " ", "4", ",", " ", "7", ",", " ", "8", ",", " ", "1", "4", ",", " ", "2", "8", ",", " ", "5", "6", " \\", "]\n\n", "###", " Step", " ", "6", ":", " Determine", " Correspond", "ing", " Values", " of", " \\(", " b", " \\", ")\n", "From", " \\(", " b", " +", " ", "7", " \\", ")", " must", " be", " one", " of", " these", " div", "isors", ",", " so", ":\n", "\\[", " b", " +", " ", "7", " =", " d", " \\", "quad", " \\", "text", "{", "where", " }", " d", " \\", "text", "{", " is", " a", " divisor", " of", " ", "5", "6", "}", " \\", "]\n", "\\[", " b", " =", " d", " -", " ", "7", " \\", "]\n\n", "But", " the", " problem", " specifies", " \\(", " b", " >", " ", "9", " \\", "),", " so", ":\n", "\\[", " d", " -", " ", "7", " >", " ", "9", " \\", "]\n", "\\[", " d", " >", " ", "1", "6", " \\", "]\n\n", "Looking", " at", " the", " div", "isors", ":\n", "-", " \\(", " d", " =", " ", "2", "8", " \\", "):", " \\(", " b", " =", " ", "2", "1", " \\", ")\n", "-", " \\(", " d", " =", " ", "5", "6", " \\", "):", " \\(", " b", " =", " ", "4", "9", " \\", ")\n\n", "Other", " div", "isors", " \\(", " d", " \\", "le", "q", " ", "1", "6", " \\", ")", " would", " give", " \\(", " b", " \\", "le", "q", " ", "9", " \\", "),", " which", " is", " invalid", ".\n\n", "###", " Verification", "\n", "I", " need", " to", " ensure", " that", " for", " these", " \\(", " b", " \\", ")", " values", ",", " the", " original", " condition", " holds", ".\n\n", "**", "For", " \\(", " b", " =", " ", "2", "1", " \\", "):", "**\n", "-", " \\(", " ", "1", "7", "_{", "2", "1", "}", " =", " ", "2", "1", " +", " ", "7", " =", " ", "2", "8", " \\", ")\n", "-", " \\(", " ", "9", "7", "_{", "2", "1", "}", " =", " ", "9", " \\", "times", " ", "2", "1", " +", " ", "7", " =", " ", "1", "8", "9", " +", " ", "7", " =", " ", "1", "9", "6", " \\", ")\n", "-", " Check", " if", " \\(", " ", "2", "8", " \\", ")", " divides", " \\(", " ", "1", "9", "6", " \\", "):", " \\(", " ", "1", "9", "6", " \\", "div", " ", "2", "8", " =", " ", "7", " \\", ").", " Exactly", ".\n\n", "**", "For", " \\(", " b", " =", " ", "4", "9", " \\", "):", "**\n", "-", " \\(", " ", "1", "7", "_{", "4", "9", "}", " =", " ", "4", "9", " +", " ", "7", " =", " ", "5", "6", " \\", ")\n", "-", " \\(", " ", "9", "7", "_{", "4", "9", "}", " =", " ", "9", " \\", "times", " ", "4", "9", " +", " ", "7", " =", " ", "4", "4", "1", " +", " ", "7", " =", " ", "4", "4", "8", " \\", ")\n", "-", " Check", " if", " \\(", " ", "5", "6", " \\", ")", " divides", " \\(", " ", "4", "4", "8", " \\", "):", " \\(", " ", "4", "4", "8", " \\", "div", " ", "5", "6", " =", " ", "8", " \\", ").", " Exactly", ".\n\n", "All", " other", " div", "isors", " \\(", " d", " \\", "le", "q", " ", "1", "6", " \\", ")", " would", " give", " \\(", " b", " \\", "le", "q", " ", "9", " \\", "),", " which", " is", " not", " allowed", ".\n\n", "###", " Sum", " of", " Valid", " B", "ases", "\n", "Only", " \\(", " b", " =", " ", "2", "1", " \\", ")", " and", " \\(", " b", " =", " ", "4", "9", " \\", ")", " satisfy", " the", " condition", " \\(", " b", " >", " ", "9", " \\", ").\n\n", "Now", ",", " sum", " them", ":\n", "\\[", " ", "2", "1", " +", " ", "4", "9", " =", " ", "7", "0", " \\", "]\n\n", "###", " Conclusion", "\n", "The", " sum", " of", " all", " integer", " bases", " \\(", " b", " >", " ", "9", " \\", ")", " for", " which", " \\(", " ", "1", "7", "_b", " \\", ")", " divides", " \\(", " ", "9", "7", "_b", " \\", ")", " is", " \\(", " \\", "boxed", "{", "7", "0", "}", " \\", ").", "<|im_end|>", "<|endoftext|>", "\n"], "step_map": [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 2, 2, 1, 3, 5, 3, 4, 6, 7, 6, 8, 9, 10, 9, 9, 11, 11, 11, 11, 12, 12, 12, 13, 14, 14, 14, 14, 16, 15, 15, 15, 17, 17, 17, 17, 18, 18, 18, 18, 19, 19, 19, 19, 20, 20, 20, 20, 21, 21, 21, 21, 22, 22, 22, 22, 23, 23, 23, 23, 24, 24, 24, 24, 25, 25, 25, 25, 26, 26, 26, 26, 27, 27, 27, 27, 28, 28, 28, 28, 29, 29, 29, 29, 30, 30, 30, 30, 31, 31, 31, 31, 32, 32, 32, 32, 33, 33, 33, 33, 34, 34, 34, 34, 35, 35, 35, 35, 36, 37, 37, 37, 39, 38, 39, 39, 40, 40, 40, 40, 41, 41, 41, 41, 42, 43, 44, 44, 45, 45, 45, 46, 47, 48, 50, 49, 51, 53, 54, 52, 55, 58, 56, 57, 59, 60, 61, 62, 63, 63, 63, 63, 65, 64, 65, 65, 66, 66, 67, 68, 69, 70, 70, 70, 71, 72, 73, 71, 74, 74, 74, 74, 75, 75, 76, 77, 78, 79, 80, 81, 82, 82, 82, 83, 84, 85, 87, 86, 88, 88, 88, 89, 90, 90, 91, 92, 94, 93, 96, 95, 99, 97, 98, 100, 101, 104, 102, 103, 105, 105, 105, 105, 106, 106, 106, 106, 107, 107, 107, 107, 108, 108, 108, 108, 109, 109, 109, 109, 110, 110, 110, 110, 111, 111, 111, 111, 112, 112, 112, 112, 113, 113, 113, 113, 114, 115, 117, 116, 120, 118, 119, 120, 121, 121, 121, 121, 124, 122, 123, 123, 125, 125, 125, 126, 128, 127, 128, 129, 131, 131, 130, 130, 132, 132, 132, 132, 133, 133, 133, 133, 134, 134, 134, 135, 136, 136, 136, 137, 138, 138, 138, 138, 139, 139, 139, 139, 140, 142, 143, 141, 144, 145, 146, 145, 149, 148, 147, 150, 151, 151, 153, 152, 155, 155, 156, 154, 157, 157, 157, 158, 159, 161, 162, 160, 164, 163, 165, 166, 167, 169, 169, 168, 170, 170, 170, 170, 171, 171, 171, 171, 172, 172, 172, 172, 173, 173, 173, 174, 175, 175, 175, 175, 176, 176, 177, 178, 179, 180, 181, 181, 182, 182, 182, 183, 184, 186, 185, 186, 187, 187, 187, 187, 188, 188, 188, 188, 190, 190, 189, 191, 192, 192, 192, 192, 193, 193, 193, 193, 194, 194, 194, 194, 195, 197, 198, 196, 199, 199, 199, 199, 200, 200, 200, 200, 201, 201, 201, 201, 202, 202, 202, 202, 203, 203, 203, 204, 205, 207, 208, 206, 209, 209, 209, 209, 210, 210, 211, 210, 212, 212, 212, 212, 213, 213, 213, 213, 214, 214, 214, 214, 215, 215, 215, 215, 216, 216, 216, 217, 218, 218, 218, 218, 220, 219, 219, 219, 221, 221, 221, 221, 222, 222, 222, 222, 223, 223, 223, 223, 224, 224, 224, 224, 225, 225, 225, 225, 226, 226, 226, 226, 227, 227, 227, 227, 228, 228, 228, 228, 229, 230, 231, 231, 232, 233, 233, 233, 234, 234, 234, 234, 235, 235, 235, 235, 236, 236, 236, 236, 237, 237, 237, 237, 238, 238, 238, 238, 239, 239, 239, 239, 240, 240, 240, 240, 241, 241, 241, 241, 242, 242, 242, 242, 245, 246, 244, 243, 250, 249, 248, 247, 251, 251, 251, 252, 256, 253, 254, 255, 260, 259, 258, 257, 261, 261, 261, 262, 263, 263, 264, 265, 266, 268, 267, 268, 269, 269, 269, 269, 270, 270, 270, 270, 271, 271, 271, 271, 272, 272, 272, 272, 273, 273, 274, 275, 276, 276, 278, 277, 279, 279, 279, 279, 280, 280, 280, 280, 282, 282, 281, 283, 284, 285, 285, 284, 286, 286, 287, 288, 289, 291, 290, 292, 293, 295, 294, 295, 296, 296, 296, 296, 297, 299, 298, 300, 301, 302, 301, 302, 303, 304, 305, 305, 306, 307, 308, 309, 311, 310, 311, 312, 313, 315, 314, 316, 317, 317, 317, 317, 318, 319, 320, 320, 321, 321, 321, 321, 322, 325, 323, 324, 326, 326, 327, 328, 329, 330, 329, 331, 332, 334, 333, 334, 335, 335, 335, 336, 337, 338, 340, 339, 341, 341, 341, 341, 342, 343, 345, 344, 346, 346, 346, 347, 350, 348, 349, 351, 353, 352, 354, 355, 356, 356, 357, 358, 360, 359, 359, 360, 361, 361, 361, 361, 362, 364, 363, 365, 366, 366, 366, 367, 368, 368, 368, 368, 369, 369, 369, 369, 370, 370, 370, 370, 371, 371, 371, 371, 372, 373, 372, 372, 374, 374, 374, 374, 375, 375, 375, 376, 378, 377, 378, 378, 380, 381, 379, 380, 382, 382, 382, 382, 383, 383, 383, 383, 384, 384, 384, 384, 385, 385, 385, 385, 386, 386, 386, 386, 387, 387, 387, 387, 388, 389, 390, 391, 395, 392, 393, 394, 396, 396, 396, 397, 398, 398, 398, 399, 401, 401, 400, 401, 402, 402, 402, 403, 404, 404, 404, 404, 405, 406, 408, 407, 409, 410, 412, 411, 413, 413, 413, 413, 414, 414, 416, 415, 417, 418, 420, 419, 421, 423, 423, 422, 424, 424, 424, 424, 425, 425, 425, 426, 427, 429, 428, 429, 430, 433, 431, 432, 434, 437, 436, 435, 438, 440, 439, 441, 442, 442, 442, 442, 444, 445, 443, 444, 447, 447, 446, 446, 448, 448, 448, 449, 451, 452, 450, 453, 454, 455, 455, 454, 456, 457, 459, 458, 460, 460, 460, 461, 463, 462, 463, 463, 464, 464, 464, 464, 465, 465, 465, 465, 466, 466, 466, 466, 467, 467, 467, 467, 468, 468, 468, 468, 469, 469, 469, 469, 470, 472, 471, 472, 473, 473, 473, 473, 474, 474, 475, 474, 476, 476, 476, 476, 477, 480, 479, 478, 481, 484, 483, 482, 486, 488, 487, 485, 489, 492, 491, 490, 495, 494, 494, 493, 497, 497, 497, 496, 498, 498, 500, 499, 501, 502, 503, 504, 505, 505, 505, 505, 506, 506, 506, 506, 507, 509, 509, 508, 510, 510, 510, 510, 511, 511, 512, 513, 514, 515, 516, 517, 518, 519, 519, 520, 521, 521, 521, 521, 522, 522, 523, 524, 527, 525, 525, 526, 528, 528, 528, 529, 530, 530, 530, 530, 531, 532, 533, 533, 534, 534, 534, 534, 535, 535, 535, 535, 536, 536, 536, 536, 537, 537, 537, 537, 538, 538, 538, 538, 539, 539, 539, 540, 541, 542, 541, 543, 544, 544, 544, 544, 545, 545, 545, 545, 546, 546, 547, 546, 548, 548, 548, 548, 549, 549, 549, 549, 551, 551, 550, 550, 552, 553, 552, 552, 554, 554, 555, 556, 557, 559, 558, 558, 561, 561, 560, 561, 562, 563, 562, 562, 565, 564, 565, 566, 567, 567, 567, 567, 568, 568, 568, 569, 570, 570, 570, 570, 571, 571, 571, 571, 572, 574, 573, 574, 575, 575, 575, 575, 576, 576, 576, 576, 578, 577, 577, 577, 579, 579, 579, 579, 580, 581, 580, 582, 583, 585, 584, 583, 586, 587, 586, 586, 589, 588, 588, 588, 590, 591, 591, 590, 592, 592, 593, 593, 594, 594, 594, 594, 595, 595, 595, 595, 596, 596, 596, 596, 599, 598, 597, 598, 600, 600, 601, 602, 603, 604, 606, 605, 607, 608, 609, 610, 611, 612, 614, 613, 615, 615, 615, 615, 616, 616, 616, 616, 617, 617, 617, 617, 618, 619, 620, 620, 621, 622, 621, 623, 624, 625, 626, 627, 628, 629, 631, 630, 633, 632, 634, 635, 638, 636, 637, 639, 640, 642, 642, 641, 643, 643, 643, 643, 644, 645, 647, 646, 648, 649, 651, 650, 653, 653, 653, 652, 654, 654, 656, 655, 657, 658, 659, 660, 661, 664, 663, 662, 665, 665, 665, 666, 667, 667, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 679, 678, 680, 681, 682, 681, 683, 685, 685, 684, 685, 686, 686, 686, 687, 688, 689, 689, 689, 690, 690, 691, 692, 696, 694, 695, 693, 697, 697, 697, 698, 699, 700, 700, 700, 701, 701, 701, 701, 702, 704, 703, 705, 708, 707, 706, 706, 709, 711, 710, 712, 713, 714, 715, 716, 717, 717, 717, 717, 718, 718, 718, 718, 719, 719, 719, 719, 720, 720, 720, 720, 721, 721, 722, 723, 725, 724, 726, 726, 727, 730, 728, 729, 731, 732, 732, 732, 733, 733, 733, 733, 735, 734, 735, 736, 737, 739, 739, 738, 740, 740, 741, 742, 743], "step_map_auto": [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372], "is_special": [false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false], "prompt_length": 57}</script>
        <script>
            const DATA = JSON.parse(document.getElementById('data').textContent);
            const pieces = DATA.pieces.map(piece => 
            piece.replace(/\f/g, '\\f').replace(/\t/g, '\\t')
                .replace(/\r/g, '\\r').replace(/\v/g, '\\v')
            );
            const steps  = DATA.step_map;  // diffusion decoding çš„ step map
            const stepsAuto = DATA.step_map_auto;  // autoregressive decoding çš„é¡ºåº step map
            const isSpec = DATA.is_special;
            const promptLength = DATA.prompt_length;

            const maxStep = Math.max(...steps);
            const maxStepAuto = Math.max(...stepsAuto);
            const promptEl = document.getElementById('prompt');
            const outputEl = document.getElementById('output');
            const outputAutoEl = document.getElementById('output-autoregressive');
            const metaParallel = document.getElementById('meta-parallel');
            const metaAutoregressive = document.getElementById('meta-autoregressive');
            let currentStep = 0;
            let currentStepAuto = 0;  // autoregressive çš„å½“å‰æ­¥æ•°

            function render(t, tAuto) {
            const fragPrompt = document.createDocumentFragment();
            const fragOutput = document.createDocumentFragment();
            const fragOutputAuto = document.createDocumentFragment();

            let revealed = 0;

            // é¦–å…ˆç»Ÿè®¡å·²ç»ç”Ÿæˆçš„ output token æ•°é‡ï¼ˆdiffusion decodingï¼‰
            for (let i = 0; i < pieces.length; i++) {
                if (steps[i] !== -1 && steps[i] <= t) {
                    revealed++;
                }
            }

            // è®¡ç®— autoregressive å·²ç”Ÿæˆçš„ token æ•°é‡ï¼ˆä½¿ç”¨ autoregressive çš„ step mapï¼‰
            let revealedAuto = 0;
            for (let i = 0; i < pieces.length; i++) {
                if (stepsAuto[i] !== -1 && stepsAuto[i] <= tAuto) {
                    revealedAuto++;
                }
            }

            // è®¡ç®—å½“å‰åº”è¯¥æ˜¾ç¤ºçš„ mask token èŒƒå›´
            // å½“å‰æ‰¹æ¬¡çš„èµ·å§‹ä½ç½® = å·²ç”Ÿæˆæ•°é‡å‘ä¸‹å–æ•´åˆ° BLOCK_SIZE çš„å€æ•°
            const currentBatchStart = Math.floor(revealed / 4) * 4;
            const currentBatchEnd = currentBatchStart + 4;

            let outputTokenIndex = 0; // output token çš„ç´¢å¼•ï¼ˆä¸åŒ…æ‹¬ promptï¼‰

            for (let i = 0; i < pieces.length; i++) {
                const span = document.createElement('span');
                const spanAuto = document.createElement('span');
                const piece = pieces[i];
                const step = steps[i];
                const stepAuto = stepsAuto[i];

                if (step === -1) {
                // Prompt tokens
                span.className = 'unmasked prompt';
                span.textContent = piece;
                fragPrompt.appendChild(span);
                } else {
                // Output tokens - Parallel decoding with mask (diffusion)
                if (step <= t) {
                    // å·²ç»ç”Ÿæˆçš„ token
                    span.className = 'unmasked' + (step === t ? ' new' : '');
                    span.textContent = piece;
                    fragOutput.appendChild(span);
                } else {
                    // æœªç”Ÿæˆçš„ token - æ£€æŸ¥æ˜¯å¦åœ¨å½“å‰æ‰¹æ¬¡çš„ mask èŒƒå›´å†…
                    if (outputTokenIndex >= currentBatchStart && outputTokenIndex < currentBatchEnd) {
                        span.className = 'masked maskToken' + (isSpec[i] ? ' special' : '');
                        span.textContent = '|<MASK>|';
                        fragOutput.appendChild(span);
                    }
                }
                
                // Autoregressive decoding - æŒ‰é¡ºåºæ˜¾ç¤ºï¼Œæ¯ step ç”Ÿæˆ 1 ä¸ª token
                if (stepAuto !== -1 && stepAuto <= tAuto) {
                    spanAuto.className = 'unmasked' + (stepAuto === tAuto ? ' new' : '');
                    spanAuto.textContent = piece;
                    fragOutputAuto.appendChild(spanAuto);
                }
                
                outputTokenIndex++;
                }
            }

            promptEl.innerHTML = '';
            promptEl.appendChild(fragPrompt);
            outputEl.innerHTML = '';
            outputEl.appendChild(fragOutput);
            outputAutoEl.innerHTML = '';
            outputAutoEl.appendChild(fragOutputAuto);
            // è‡ªåŠ¨æ»šåŠ¨åˆ°åº•éƒ¨ï¼Œæ˜¾ç¤ºæœ€æ–°å†…å®¹
            outputEl.scrollTop = outputEl.scrollHeight;
            outputAutoEl.scrollTop = outputAutoEl.scrollHeight;
            
            // å¹¶è¡Œè§£ç é€Ÿåº¦ï¼šæ¯æ­¥ç”Ÿæˆçš„ token æ•° = revealed / (t + 1)
            const parallelSpeed = t >= 0 ? (revealed / (t + 1)).toFixed(2) : 0;
            const parallelSteps = t >= 0 ? t + 1 : 0;
            metaParallel.textContent = `Generation speed: ${parallelSpeed} tokens/step | Tokens: ${revealed} | Steps: ${parallelSteps}`;
            
            // è‡ªå›žå½’é€Ÿåº¦ï¼šæ¯æ­¥ç”Ÿæˆ 1 ä¸ª token
            const autoSteps = tAuto >= 0 ? tAuto + 1 : 0;
            metaAutoregressive.textContent = `Generation speed: 1.00 tokens/step | Tokens: ${revealedAuto} | Steps: ${autoSteps}`;
            }

            function autoPlay() {
            render(currentStep, currentStepAuto);
            
            // diffusion decoding æ¯ä¸ª tick éƒ½æ›´æ–°
            if (currentStep <= maxStep) {
                currentStep++;
            }
            
            // autoregressive æ¯ SPEED_RATIO ä¸ª tick æ›´æ–°ä¸€æ¬¡ï¼ˆé€Ÿåº¦æ¯” diffusion æ…¢ï¼‰
            if (currentStepAuto <= maxStepAuto) {
                currentStepAuto++;
            }
            
            // å½“ä¸¤ä¸ªéƒ½å®Œæˆæ—¶ï¼Œç­‰å¾…åŽé‡æ–°å¼€å§‹
            if (currentStep > maxStep && currentStepAuto > maxStepAuto) {
                setTimeout(() => {
                currentStep = 0;
                currentStepAuto = 0;
                autoPlay();
                }, 2000);
            } else {
                setTimeout(autoPlay, 100);
            }
            }
            autoPlay();
            
        </script>
        </body>
    </div>
  </div>
</section>



<!-- Method Section -->
<section class="hero is-small is-light" style="padding: 5rem 1.5rem;">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered" style="font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;">
      Method
    </h2>

    <div class="content has-text-justified" style="margin-bottom: 2rem; font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; font-size: 1.05rem; line-height: 1.7; color: #333;">
      <p style="margin-bottom: 1.5rem; text-align: justify;">
        We develop and release an open-source <strong>diffusion post-training framework</strong> for DLLMs, and train <strong style="font-weight: 600;">DiRL-8B-Instruct</strong> based on <strong>SDAR-8B-Chat</strong> through two stages: 
        <em>Supervised Fine-Tuning (SFT)</em> and <em>Reinforcement Learning (RL)</em>.
      </p>

      <div style="margin-bottom: 1.5rem; padding-left: 1rem; border-left: 3px solid #3273dc;">
        <h3 style="font-size: 1.15rem; font-weight: 600; margin-bottom: 0.75rem; color: #3273dc;">
          Stage 1: Supervised Fine-Tuning (SFT)
        </h3>
        <p style="margin-bottom: 0; text-align: justify;">
          We prepare a proprietary, high-quality mathematical dataset with <strong>2K</strong> samples and a generation length of <strong>8K</strong> tokens. We adopt a random-masking strategy to construct the training data for model fine-tuning.
        </p>
      </div>

      <div style="margin-bottom: 1.5rem; padding-left: 1rem; border-left: 3px solid #48c774;">
        <h3 style="font-size: 1.15rem; font-weight: 600; margin-bottom: 0.75rem; color: #48c774;">
          Stage 2: Reinforcement Learning (RL)
        </h3>
        <p style="margin-bottom: 0; text-align: justify;">
          We design an RL algorithm -- <strong>DiPO</strong>, and train the model with a generation length of <strong>8K</strong>. We achieve an unbiased implementation of RL theory, ensuring complete consistency between the optimization objective and the actual training process. Additionally, during the rollout phase, we adopt dynamic sampling from DAPO to filter out data with zero advantage standard deviation.
        </p>
      </div>

      <p style="text-align: justify;">
        Through this two-stage training pipeline, we successfully train <strong>DiRL-8B-Instruct</strong>, a high-performance diffusion language model for mathematical reasoning.
      </p>
    </div>
  </div>
</section>


<!-- Performance Section -->
<section class="hero is-small" style="padding: 5rem 1.5rem;">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered" style="font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;">Performance</h2>

      <div class="content-has-text-justified" style="margin-bottom: 2rem; font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; font-size: 1.05rem; line-height: 1.7;">
        <p style="margin-bottom: 1rem; text-align: justify;">
          <strong style="font-weight: 600;">DiRL-8B-Instruct</strong> achieves state-of-the-art results among DLLMs across mathematical reasoning benchmarks. Highlights include <strong style="font-weight: 600;">83.05%</strong> on MATH500 (surpassing the base model by <strong style="font-weight: 600;">+11.20%</strong>), <strong style="font-weight: 600;">20.63%</strong> on AIME2024 and <strong style="font-weight: 600;">20.83%</strong> on AIME2025 (dramatically outperforming all baselines), and <strong style="font-weight: 600;">46.40%</strong> on OlympiadBench. Our 8B model achieves performance comparable to or exceeding much larger 32B models on most benchmarks.
        </p>
      </div>


      <style>
        /* ä¸‰çº¿è¡¨æ ·å¼ï¼ˆé€‚é… Bulmaï¼‰ */
        .three-line-table {
          border-collapse: collapse;
          width: 100%;
          font-family: "Helvetica Neue", Arial, sans-serif;
          font-size: 0.95rem;
          text-align: center;
        }
        .three-line-table th,
        .three-line-table td {
          padding: 0.55rem 0.8rem;
        }

        /* é¡¶çº¿ï¼ˆç²—ï¼‰*/
        .three-line-table thead tr th {
          border-top: 2px solid #0a0a0a;
        }
        /* ä¸­çº¿ï¼ˆè¾ƒç»†ï¼‰*/
        .three-line-table thead {
          border-bottom: 1px solid #0a0a0a;
        }
        /* åº•çº¿ï¼ˆç²—ï¼‰*/
        .three-line-table tbody tr:last-child td {
          border-bottom: 2px solid #0a0a0a;
        }

        /* åŽ»æŽ‰ç«–çº¿æ•ˆæžœ â€”â€” åªä¿ç•™ä¸‰æ¡æ¨ªçº¿ï¼ˆé»˜è®¤æ²¡æœ‰ç«–çº¿ï¼‰ */
        .three-line-table th,
        .three-line-table td {
          border-left: none;
          border-right: none;
        }

        /* å°å¯¹é½ä¸Žå­—ä½“æ ·å¼ */
        .three-line-table thead th {
          font-weight: 600;
          padding-top: 0.9rem;
          padding-bottom: 0.6rem;
        }
        .three-line-table tbody td {
          color: #222;
        }

        /* é«˜äº® ours è¡Œï¼Œå¹¶åŠ ç²—å…¶æœ€ä½³å€¼ */
        .three-line-table .ours {
          background: rgba(90,160,80,0.06);
        }
        .three-line-table .best {
          font-weight: 700;
        }
        
        /* æå‡æ ‡ç­¾æ ·å¼ */
        .gain-badge {
          display: inline-block;
          margin-left: 0.35rem;
          padding: 0.1rem 0.35rem;
          font-size: 0.75rem;
          font-weight: 600;
          color: #28a745;
          background: #d4edda;
          border-radius: 3px;
          vertical-align: middle;
        }

        /* å“åº”å¼ï¼šå°å±å¹•å­—ä½“å¾®è°ƒ */
        @media (max-width: 640px) {
          .three-line-table th, .three-line-table td { padding: 0.45rem 0.4rem; font-size: 0.88rem; }
        }
      </style>

      <div class="table-wrapper" style="margin-top:1rem;">
        <table class="three-line-table">
          <thead>
            <tr>
              <th>Model</th>
              <th>MATH500</th>
              <th>GSM8K</th>
              <th>AIME2024</th>
              <th>AIME2025</th>
              <th>OlympiadBench</th>
              <th>Average</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Qwen2.5-7B-Instruct</td>
              <td>73.78</td>
              <td>89.78</td>
              <td>8.96</td>
              <td>5.63</td>
              <td>36.58</td>
              <td>42.95</td>
            </tr>
            <tr>
              <td>Qwen2.5-32B-Instruct</td>
              <td>81.13</td>
              <td class="best">94.03</td>
              <td>12.92</td>
              <td>11.88</td>
              <td>45.65</td>
              <td>49.12</td>
            </tr>
            <tr>
              <td>SDAR-8B-Chat</td>
              <td>71.85</td>
              <td>89.87</td>
              <td>9.17</td>
              <td>9.38</td>
              <td>36.03</td>
              <td>43.26</td>
            </tr>
            <tr>
              <td>Trado-8B-Instruct</td>
              <td>75.59</td>
              <td>91.06</td>
              <td>11.67</td>
              <td>15.00</td>
              <td>40.32</td>
              <td>46.73</td>
            </tr>
            <tr class="ours">
              <td><strong>DiRL-8B-Instruct (ours)</strong></td>
              <td class="best">83.05<span class="gain-badge">+11.20</span></td>
              <td>93.03<span class="gain-badge">+3.16</span></td>
              <td class="best">20.63<span class="gain-badge">+11.46</span></td>
              <td class="best">20.83<span class="gain-badge">+11.45</span></td>
              <td class="best">46.40<span class="gain-badge">+10.37</span></td>
              <td class="best">52.79<span class="gain-badge">+9.53</span></td>
            </tr>
          </tbody>
        </table>
      </div>
      </div>
    </div>
  </div>
</section>
<!-- End Performance Section -->


<!-- Citation Section -->
<section class="hero is-small is-light" style="padding: 5rem 1.5rem;">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered" style="margin-bottom: 2rem; font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;">Citation</h2>
    <div class="content" style="font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; font-size: 1.05rem;">
      <p style="margin-bottom: 1.5rem;">If you find our work helpful, please consider citing:</p>
      <pre style="background-color: #f5f5f5; padding: 1.5rem; border-radius: 8px; overflow-x: auto; border: 1px solid #e0e0e0;"><code>@misc{zhu2025dirl,
  title={DiRL: An Efficient Post-Training Framework for Diffusion Language Models},
  author={Ying Zhu and Jiaxin Wan and Xiaoran Liu and Siyang He and Qiqi Wang and Xu Guo and Tianyi Liang and Zengfeng Huang and Ziwei He and Xipeng Qiu},
  year={2025},
  eprint={2512.22234},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2512.22234}
}</code></pre>
    </div>
  </div>
</section>
<!-- End Citation Section -->
